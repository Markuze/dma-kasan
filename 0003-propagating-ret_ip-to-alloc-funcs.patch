From d73588602d1fb872e8fdf2db71743fba1b68ffbd Mon Sep 17 00:00:00 2001
From: Markuze Alex <amarkuze@vmware.com>
Date: Fri, 29 May 2020 10:20:52 +0000
Subject: [PATCH 3/4] propagating ret_ip to alloc funcs

---
 include/linux/kasan.h | 10 +++++-----
 include/linux/slab.h  |  1 +
 mm/kasan/common.c     | 35 ++++++++++++++++++++---------------
 mm/kasan/kasan.h      |  2 +-
 mm/slab.c             |  8 ++++----
 mm/slab.h             |  4 ++--
 mm/slab_common.c      |  2 +-
 mm/slub.c             | 16 ++++++++--------
 8 files changed, 42 insertions(+), 36 deletions(-)

diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index b40ea104d..91012007e 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -57,12 +57,12 @@ void * __must_check kasan_kmalloc_large(const void *ptr, size_t size,
 void kasan_kfree_large(void *ptr, unsigned long ip);
 void kasan_poison_kfree(void *ptr, unsigned long ip);
 void * __must_check kasan_kmalloc(struct kmem_cache *s, const void *object,
-					size_t size, gfp_t flags);
+					size_t size, gfp_t flags, unsigned long);
 void * __must_check kasan_krealloc(const void *object, size_t new_size,
-					gfp_t flags);
+					gfp_t flags, unsigned long);
 
 void * __must_check kasan_slab_alloc(struct kmem_cache *s, void *object,
-					gfp_t flags);
+					gfp_t flags, unsigned long);
 bool kasan_slab_free(struct kmem_cache *s, void *object, unsigned long ip);
 
 struct kasan_cache {
@@ -123,13 +123,13 @@ static inline void *kasan_kmalloc(struct kmem_cache *s, const void *object,
 	return (void *)object;
 }
 static inline void *kasan_krealloc(const void *object, size_t new_size,
-				 gfp_t flags)
+				 gfp_t flags, unsigned long)
 {
 	return (void *)object;
 }
 
 static inline void *kasan_slab_alloc(struct kmem_cache *s, void *object,
-				   gfp_t flags)
+				   gfp_t flags, unsigned long ip)
 {
 	return object;
 }
diff --git a/include/linux/slab.h b/include/linux/slab.h
index 11b45f7ae..49867ba59 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -178,6 +178,7 @@ void memcg_destroy_kmem_caches(struct mem_cgroup *);
 /*
  * Common kmalloc functions provided by all allocators
  */
+
 void * __must_check __krealloc(const void *, size_t, gfp_t);
 void * __must_check krealloc(const void *, size_t, gfp_t);
 void kfree(const void *);
diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 15828a1bd..58679da40 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -448,13 +448,13 @@ static bool __kasan_slab_free(struct kmem_cache *cache, void *object,
 	/* RCU slabs could be legally used after free within the RCU period */
 	if (unlikely(cache->flags & SLAB_TYPESAFE_BY_RCU))
 		return false;
-
-	shadow_byte = READ_ONCE(*(s8 *)kasan_mem_to_shadow(object));
-	if (shadow_invalid(tag, shadow_byte)) {
-		kasan_report_invalid_free(tagged_object, ip);
-		return true;
-	}
-
+/*
+*	shadow_byte = READ_ONCE(*(s8 *)kasan_mem_to_shadow(object));
+*	if (shadow_invalid(tag, shadow_byte)) {
+*		kasan_report_invalid_free(tagged_object, ip);
+*		return true;
+*	}
+*/
 	rounded_up_size = round_up(cache->object_size, KASAN_SHADOW_SCALE_SIZE);
 	kasan_poison_shadow(object, rounded_up_size, 0);
 
@@ -474,11 +474,13 @@ bool kasan_slab_free(struct kmem_cache *cache, void *object, unsigned long ip)
 }
 
 static void *__kasan_kmalloc(struct kmem_cache *cache, const void *object,
-				size_t size, gfp_t flags, bool keep_tag)
+				size_t size, gfp_t flags, bool keep_tag,
+				unsigned long ret_ip)
 {
 	unsigned long redzone_start;
 	unsigned long redzone_end;
 	struct page *page = virt_to_page(object);
+	bool write;
 	u8 tag;
 
 	if (gfpflags_allow_blocking(flags))
@@ -508,7 +510,10 @@ static void *__kasan_kmalloc(struct kmem_cache *cache, const void *object,
 #else
 
 	//TODO: Check mapping flags and report.
-	//	kasan_report(addr, size, write, ret_ip); - check if page has dma write access
+/*	write = test_bit(PG_DMA_W, &page->flags);
+*	if (test_bit(PG_DMA_R, &page->flags) || write)
+*		kasan_report(addr, size, write, ret_ip); - check if page has dma write access
+*/
 	kasan_poison_shadow(object, size, DMA_KASAN_ALLOC);
 /*
 TODO: Need to propagate ret_ip
@@ -521,15 +526,15 @@ TODO: Need to propagate ret_ip
 }
 
 void * __must_check kasan_slab_alloc(struct kmem_cache *cache, void *object,
-					gfp_t flags)
+					gfp_t flags, unsigned long ret_ip)
 {
-	return __kasan_kmalloc(cache, object, cache->object_size, flags, false);
+	return __kasan_kmalloc(cache, object, cache->object_size, flags, false, ret_ip);
 }
 
 void * __must_check kasan_kmalloc(struct kmem_cache *cache, const void *object,
-				size_t size, gfp_t flags)
+				size_t size, gfp_t flags, unsigned long ret_ip)
 {
-	return __kasan_kmalloc(cache, object, size, flags, true);
+	return __kasan_kmalloc(cache, object, size, flags, true, ret_ip);
 }
 EXPORT_SYMBOL(kasan_kmalloc);
 
@@ -560,7 +565,7 @@ void * __must_check kasan_kmalloc_large(const void *ptr, size_t size,
 	return (void *)ptr;
 }
 
-void * __must_check kasan_krealloc(const void *object, size_t size, gfp_t flags)
+void * __must_check kasan_krealloc(const void *object, size_t size, gfp_t flags, unsigned long ret_ip)
 {
 	struct page *page;
 
@@ -573,7 +578,7 @@ void * __must_check kasan_krealloc(const void *object, size_t size, gfp_t flags)
 		return kasan_kmalloc_large(object, size, flags);
 	else
 		return __kasan_kmalloc(page->slab_cache, object, size,
-						flags, true);
+						flags, true, ret_ip);
 }
 
 void kasan_poison_kfree(void *ptr, unsigned long ip)
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index 900c9b9a1..ccb6860b0 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -12,7 +12,7 @@
 #define KASAN_TAG_INVALID	0xFE /* inaccessible memory tag */
 #define KASAN_TAG_MAX		0xFD /* maximum value for random tags */
 
-#define DMA_KASAN_ALLOC		0x3F;/* We leave two bits for dma-map values */
+#define DMA_KASAN_ALLOC		0x3F /* We leave two bits for dma-map values */
 
 #ifdef CONFIG_KASAN_GENERIC
 #define KASAN_FREE_PAGE         0xFF  /* page was freed */
diff --git a/mm/slab.c b/mm/slab.c
index 91c1863df..2cb6e185e 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -3328,7 +3328,7 @@ slab_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid,
 	if (unlikely(flags & __GFP_ZERO) && ptr)
 		memset(ptr, 0, cachep->object_size);
 
-	slab_post_alloc_hook(cachep, flags, 1, &ptr);
+	slab_post_alloc_hook(cachep, flags, 1, &ptr, caller);
 	return ptr;
 }
 
@@ -3385,7 +3385,7 @@ slab_alloc(struct kmem_cache *cachep, gfp_t flags, unsigned long caller)
 	if (unlikely(flags & __GFP_ZERO) && objp)
 		memset(objp, 0, cachep->object_size);
 
-	slab_post_alloc_hook(cachep, flags, 1, &objp);
+	slab_post_alloc_hook(cachep, flags, 1, &objp, caller);
 	return objp;
 }
 
@@ -3592,13 +3592,13 @@ int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,
 		for (i = 0; i < size; i++)
 			memset(p[i], 0, s->object_size);
 
-	slab_post_alloc_hook(s, flags, size, p);
+	slab_post_alloc_hook(s, flags, size, p, _RET_IP_);
 	/* FIXME: Trace call missing. Christoph would like a bulk variant */
 	return size;
 error:
 	local_irq_enable();
 	cache_alloc_debugcheck_after_bulk(s, flags, i, p, _RET_IP_);
-	slab_post_alloc_hook(s, flags, i, p);
+	slab_post_alloc_hook(s, flags, i, p, _RET_IP_);
 	__kmem_cache_free_bulk(s, i, p);
 	return 0;
 }
diff --git a/mm/slab.h b/mm/slab.h
index 384105318..de65d4ba2 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -431,13 +431,13 @@ static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,
 }
 
 static inline void slab_post_alloc_hook(struct kmem_cache *s, gfp_t flags,
-					size_t size, void **p)
+					size_t size, void **p, unsigned long ip)
 {
 	size_t i;
 
 	flags &= gfp_allowed_mask;
 	for (i = 0; i < size; i++) {
-		p[i] = kasan_slab_alloc(s, p[i], flags);
+		p[i] = kasan_slab_alloc(s, p[i], flags, ip);
 		/* As p[i] might get tagged, call kmemleak hook after KASAN. */
 		kmemleak_alloc_recursive(p[i], s->object_size, 1,
 					 s->flags, flags);
diff --git a/mm/slab_common.c b/mm/slab_common.c
index f9d89c1b5..c389a253f 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -1508,7 +1508,7 @@ static __always_inline void *__do_krealloc(const void *p, size_t new_size,
 		ks = ksize(p);
 
 	if (ks >= new_size) {
-		p = kasan_krealloc((void *)p, new_size, flags);
+		p = kasan_krealloc((void *)p, new_size, flags, _RET_IP_);
 		return (void *)p;
 	}
 
diff --git a/mm/slub.c b/mm/slub.c
index dc777761b..a3ce1c88d 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -2755,7 +2755,7 @@ static __always_inline void *slab_alloc_node(struct kmem_cache *s,
 	if (unlikely(gfpflags & __GFP_ZERO) && object)
 		memset(object, 0, s->object_size);
 
-	slab_post_alloc_hook(s, gfpflags, 1, &object);
+	slab_post_alloc_hook(s, gfpflags, 1, &object, addr);
 
 	return object;
 }
@@ -2782,7 +2782,7 @@ void *kmem_cache_alloc_trace(struct kmem_cache *s, gfp_t gfpflags, size_t size)
 {
 	void *ret = slab_alloc(s, gfpflags, _RET_IP_);
 	trace_kmalloc(_RET_IP_, ret, size, s->size, gfpflags);
-	ret = kasan_kmalloc(s, ret, size, gfpflags);
+	ret = kasan_kmalloc(s, ret, size, gfpflags, _RET_IP_);
 	return ret;
 }
 EXPORT_SYMBOL(kmem_cache_alloc_trace);
@@ -2810,7 +2810,7 @@ void *kmem_cache_alloc_node_trace(struct kmem_cache *s,
 	trace_kmalloc_node(_RET_IP_, ret,
 			   size, s->size, gfpflags, node);
 
-	ret = kasan_kmalloc(s, ret, size, gfpflags);
+	ret = kasan_kmalloc(s, ret, size, gfpflags, _RET_IP_);
 	return ret;
 }
 EXPORT_SYMBOL(kmem_cache_alloc_node_trace);
@@ -3182,11 +3182,11 @@ int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,
 	}
 
 	/* memcg and kmem_cache debug support */
-	slab_post_alloc_hook(s, flags, size, p);
+	slab_post_alloc_hook(s, flags, size, p, _RET_IP_);
 	return i;
 error:
 	local_irq_enable();
-	slab_post_alloc_hook(s, flags, i, p);
+	slab_post_alloc_hook(s, flags, i, p, _RET_IP_);
 	__kmem_cache_free_bulk(s, i, p);
 	return 0;
 }
@@ -3383,7 +3383,7 @@ static void early_kmem_cache_node_alloc(int node)
 	init_tracking(kmem_cache_node, n);
 #endif
 	n = kasan_kmalloc(kmem_cache_node, n, sizeof(struct kmem_cache_node),
-		      GFP_KERNEL);
+		      GFP_KERNEL, _RET_IP_);
 	page->freelist = get_freepointer(kmem_cache_node, n);
 	page->inuse = 1;
 	page->frozen = 0;
@@ -3798,7 +3798,7 @@ void *__kmalloc(size_t size, gfp_t flags)
 
 	trace_kmalloc(_RET_IP_, ret, size, s->size, flags);
 
-	ret = kasan_kmalloc(s, ret, size, flags);
+	ret = kasan_kmalloc(s, ret, size, flags, _RET_IP_);
 
 	return ret;
 }
@@ -3842,7 +3842,7 @@ void *__kmalloc_node(size_t size, gfp_t flags, int node)
 
 	trace_kmalloc_node(_RET_IP_, ret, size, s->size, flags, node);
 
-	ret = kasan_kmalloc(s, ret, size, flags);
+	ret = kasan_kmalloc(s, ret, size, flags, _RET_IP_);
 
 	return ret;
 }
-- 
2.20.1

